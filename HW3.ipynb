{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте текст \"Литературных анекдотов\". Напишите функцию, которая будет читать файл, лемматизировать текст с помощью pymystem3 и записывать результат в новый файл. У функции должно бы два аргумента: путь к исходному файлу и путь к файлу с лемматизированным текстом. Вызов функции тоже должен быть прописан в решении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем Mystem, os, re\n",
    "from pymystem3 import Mystem\n",
    "import os\n",
    "import re\n",
    "\n",
    "# пишем функцию с двумя аргументами:путь к исходному файлу и пусть к файлу с лемматизированным текстом \n",
    "def moya_funсtion(path_to_file, path_to_lemma_file):\n",
    "    with open(path_to_file, \"r\", encoding=\"utf-8\") as f: # открываем файл с анекдотами\n",
    "        anecdot = f.read() # читаем файл с анекдотами\n",
    "\n",
    "    anecdot_without_punctuation = re.sub(\"[^А-Яа-я ]\",\"\",anecdot.replace(\"\\n\",\" \").replace(\"-\",\" \")) # очищаем анекдоты от пунктуации: все, что не буквы, заменяем на ничего; перенос строки на пробел; тире на пробел \n",
    "    anecdot_without_punctuation_and_spaces = re.sub(\" +\",\" \",anecdot_without_punctuation) # заменяем двойной пробел на одинарный\n",
    "\n",
    "    lemmas = m.lemmatize(anecdot_without_punctuation_and_spaces) # лемматизируем текст, очищенный от пунктцации и двойных пробелов\n",
    "\n",
    "# записываем лемматизированный текст в новый файл \n",
    "    with open(path_to_lemma_file, \"w\") as p: \n",
    "        for f in lemmas:\n",
    "            p.write(f)\n",
    "\n",
    "m = Mystem()\n",
    "\n",
    "# задаем пути к аргументам функции   \n",
    "moya_funсtion(\"Anec.txt\",\"Lemmas.txt\")\n",
    "\n",
    "# задаем новую переменную для прочитанного лемматизированного текста\n",
    "with open('Lemmas.txt', 'r') as g: \n",
    "    lmm = g.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистите лемматизированный текст от стоп-слов и посчитайте ipm для оставшихся. Выведите 20 самых частотных по этому параметру слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('пушкин', 35520.77062010837), ('толстой', 19867.549668874173), ('гоголь', 18663.455749548462), ('однажды', 17459.36183022276), ('лев', 15051.173991571344), ('любить', 12040.939193257074), ('достоевский', 12040.939193257074), ('тургенев', 9632.75135460566), ('ребенок', 9030.704394942806), ('царствие', 9030.704394942806), ('небесный', 9030.704394942806), ('окно', 7224.563515954244), ('тверской', 7224.563515954244), ('бульвар', 7224.563515954244), ('приходить', 7224.563515954244), ('лермонтов', 7224.563515954244), ('федор', 6622.516556291391), ('михайлович', 6622.516556291391), ('идти', 6020.469596628537), ('герцен', 6020.469596628537)]\n"
     ]
    }
   ],
   "source": [
    "# импортируем Counter\n",
    "from collections import Counter\n",
    "\n",
    "with open(\"Lemmas.txt\", \"r\") as g: \n",
    "    lmm = g.read().split() # читает лемматизированный текст и разбиваем его по пробелам\n",
    "with open(\"rus_stopwords.txt\", \"r\", encoding=\"UTF-8\") as g: \n",
    "    stop_words = set(g.read().split(\"\\n\")) # читаем файл со стоп-словами, составленный Оксаной Владимировной, разделяем его и создаем множество\n",
    "\n",
    "# lmm_without_stops - это список\n",
    "lmm_without_stops = []\n",
    "for k in lmm:\n",
    "    if k not in stop_words:\n",
    "        lmm_without_stops.append(k) # если слова нет в файле со стоп-словами, добавляем его в lmm_without_stops\n",
    "\n",
    "\n",
    "# считаем ipm для оставшихся слов - тех, которые не в стоп-словах\n",
    "\n",
    "# считаем длину текста без стоп-слов\n",
    "dlina_text = len(lmm_without_stops)\n",
    "\n",
    "# вычисляем частотные слова\n",
    "counts = Counter(lmm_without_stops)\n",
    "\n",
    "# создаем пустой словарь\n",
    "slovar_lmm = dict()\n",
    "\n",
    "# достаем из словаря все пары ключей со значениями\n",
    "for word, count in counts.items():\n",
    "    slovar_lmm[word] = count/dlina_text*1000000 # словарь, в котором ключ - часть речи, а значение - ipm\n",
    "\n",
    "# сортируем словарь, чтобы вывести первые двадцать по этому параметру слов\n",
    "sorted_counts = sorted(slovar_lmm.items(), key=lambda x: x[1], reverse=True)\n",
    "#выводим первые двадцать значений\n",
    "print(sorted_counts[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделайте полный морфологический разбор исходного текста. Напишите регулярное выражение, которое будет извлекать из тега только часть речи. Пройдитесь циклом по списку с разборами, который выдал pymystem3, извлекая из каждого разбора форму слова и его часть речи и записывая их в новый словарь (форма -- ключ, часть речи -- значение). Посчитайте абсолютную частоту для всех частей речи, а затем относительнную частоту (абсолютная частота / длина текста в словах)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Anec.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    anecdot = f.read()\n",
    "\n",
    "    anecdot_without_punctuation = re.sub(\"[^А-Яа-я ]\",\"\",anecdot.replace(\"\\n\",\" \").replace(\"-\",\" \"))\n",
    "    anecdot_without_punctuation_and_spaces = re.sub(\" +\",\" \",anecdot_without_punctuation)\n",
    "\n",
    "# делаем морфологический разбор очищенного от пунктуации и двойных пробелов текста\n",
    "analized = m.analyze(anecdot_without_punctuation_and_spaces) \n",
    "\n",
    "form_part = dict() #form_part - словарь, ключ - форма, часть речи - значение\n",
    "parts_of_speech = dict() #parts_of_speech - словарь, ключ - часть речи, значение - абсолютная частота\n",
    "words_count = 0 # длина текста в словах\n",
    "\n",
    "for v in analized:\n",
    "    if (v[\"text\"] != \" \") and (v[\"text\"] != \"\\n\") and (len(v[\"analysis\"])>0): # если v не являтся пробелом, переносом строки и длина ключа больше 0,\n",
    "        part_of_speech = re.match(\"[A-Z]+\",v[\"analysis\"][0][\"gr\"]).group() # поиск части речи в морфологическом разборе с помощью регулярного выражения\n",
    "        form_part[v[\"text\"]] = part_of_speech # заполняем словарь, где значение - часть речи, найденная с помощью регулярного выражения\n",
    "        \n",
    "        if part_of_speech in parts_of_speech: # если часть речи есть в словаре,\n",
    "            parts_of_speech[part_of_speech]+=1 # увеличить значение на единицу\n",
    "        else:\n",
    "            parts_of_speech[part_of_speech]=1 # создание переменной, если ее еще не существует в словаре\n",
    "        \n",
    "        words_count+=1 # по прохождению цикла увеличивать значение длины текста в словах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PR': 247, 'S': 823, 'V': 592, 'A': 128, 'ADV': 181, 'SPRO': 250, 'CONJ': 263, 'PART': 148, 'ADVPRO': 69, 'APRO': 78, 'ANUM': 2, 'INTJ': 8, 'NUM': 10}\n",
      "{'PR': 0.08824580207216863, 'S': 0.29403358342265096, 'V': 0.21150410861021793, 'A': 0.045730618077884956, 'ADV': 0.0646659521257592, 'SPRO': 0.08931761343336907, 'CONJ': 0.09396212933190425, 'PART': 0.05287602715255448, 'ADVPRO': 0.02465166130760986, 'APRO': 0.027867095391211148, 'ANUM': 0.0007145409074669524, 'INTJ': 0.0028581636298678098, 'NUM': 0.0035727045373347625}\n"
     ]
    }
   ],
   "source": [
    "#абсолютные частоты\n",
    "print(parts_of_speech)\n",
    "\n",
    "#отностильные частоты\n",
    "relative_frequency = dict()\n",
    "for k in parts_of_speech:\n",
    "    relative_frequency[k] = (parts_of_speech[k]/words_count)\n",
    "    \n",
    "print(relative_frequency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
